\documentclass{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{thmtools}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{float}
\usepackage{hyperref}
% \usepackage{bbold}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage[dvipsnames]{xcolor}
\usepackage{tcolorbox}
\usepackage{notes}  % Include the custom style file
\usepackage[dvipsnames]{xcolor}
% ------------------------------------------------------------------------------

\begin{document}

% ------------------------------------------------------------------------------
% Cover Page and ToC
% ------------------------------------------------------------------------------
\title{ \normalsize \textsc{}
\\ [2.0cm]
\HRule{1.5pt} \\
\LARGE \textbf{\textit{Asymptotic Statistics} \\ \Large{by VAN DER VAART}
    \HRule{2.0pt} \\ \vspace*{10\baselineskip}}
}
\date{5/24/2024 - ?}
\author{Scribed by:
    \textbf{Sida Li}}

\maketitle
\newpage

\tableofcontents
\newpage

% ------------------------------------------------------------------------------
% Chapter 1
% ------------------------------------------------------------------------------

\section{Prelim: Probability and Measure}
\begin{note}
    This part follows Chapter 1 in Keener's \textit{Theoretical Statistics} textbook. It is meant to introduce some basic concepts that will be assumed throughout rest of the note. I also bring out necessary notations from here. \textbf{Precise definitions are usually ommitted, but can be found in the textbook.}
\end{note}

\subsection{Measure and Probability Space}

We start by refreshing the rigorous definition of \textbf{measure space}, then proceed with examples.

Given a set $\mathcal{X}$, a $\sigma$-\textbf{algebra} $\mathcal{A}$ is a collection of subsets of $\mathcal{X}$ that (1) contains $\mathcal{X}$ and the empty set $\emptyset$, (2) is closed under complements, and (3) is closed under countable unions.

A \textbf{measure} $\mu:\mathcal{A} \rightarrow [0, \infty]$ is a function that assigns a non-negative real number to each element in $\mathcal{A}$, such that $\mu(\emptyset) = 0$ and $\mu$ is countably additive, i.e. for any disjoint sequence of sets $\{A_i\}_{i=1}^{\infty}$,
\begin{equation}
    \mu\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} \mu(A_i)
\end{equation}
The triple $(\mathcal{X}, \mathcal{A}, \mu)$ is called a \textbf{measure space}. A measure $\nu$ such that $\nu(\mathcal{X}) = 1$ is called a \textbf{probability measure}, and then we can define a \textbf{probability space} $(\mathcal{X}, \mathcal{A}, \nu)$.

\subsection{Lebesgue Measure and Why We Care}

Many statistics textbook, including Keener's, assures the readers that ``measure theory is not needed'' -- the motivation behind this simplification is that most of the time, we are working with the \textbf{Lebesgue measure} on $\mathbb{R}^d$. Lebesgue measure is compatible with the usual notion of length, area, and volume (in 1, 2, and 3-dimensions), and is defined on the \textbf{Borel $\sigma$-algebra} $\mathcal{B}(\mathbb{R}^d)$, which is the smallest $\sigma$-algebra containing all open sets in $\mathbb{R}^d$.

Another benefits of Lebesgue measure is its connection with our usual notion of integration. If a function $f$ is \textbf{Lebesgue integrable} (whose definition is detailed \href{https://math.stackexchange.com/questions/1716526/what-does-it-mean-to-say-that-a-function-is-integrable-with-respect-to-a-measure}{here}), then its integral is:
\begin{equation}
    \int_{\mathbb{R}^d} f(x)\, d\mu(x) = \int_{\mathbb{R}^d} f(x)\, dx
\end{equation}
where the RHS is the familiar Riemann integral. Throughout this notes, we consider all measures $\nu$ on $\mathbb{R}^d$\footnote{we also assume $\nu$ to be $\sigma$-finite, whose definition is skipped} to be \textbf{absolutely continuous} with respect to the Lebesgue measure $\mu$, i.e. $\mu(A) = 0$ implies $\nu(A) = 0$. Then by \textbf{Radonâ€“Nikodym's theorem}, we can write
\begin{equation}
    \int_{\mathbb{R}^d} f(x)\, d\nu(x) = \int_{\mathbb{R}^d} f(x) \frac{d\nu}{d\mu}(x)\, d\mu(x) = \int_{\mathbb{R}^d} f(x)\nu(x) \,dx
\end{equation}
for $\nu$-integrable functions $f$. Hopefully this clarifies why we can safely ignore measure theory in most of the (basic) statistics literature.
\newpage
% ------------------------------------------------------------------------------
% Chapter 2
% ------------------------------------------------------------------------------
\section{Convergence of Random Variables}

\begin{note}
    This part is largely following the main definitions and proofs in Chapter 2 of van der Vaart's book.
\end{note}
% ------------------------------------------------------------------------------

\subsection{Modes of Convergence}

We start by defining the different modes of convergence for random variables. Let $\{X_n\}_{n=1}^{\infty}$ be a sequence of random variables, and $X$ be another random variable.

\begin{itemize}
    \item We say that $X_n$ \textbf{converges in distribution} to $X$ if
          \begin{equation}
              \lim_{n \rightarrow \infty} P(X_n \leq x) = P(X \leq x)
          \end{equation}
          for any $x$ such that the mapping $x \to P(X \leq x)$ is continuous. We denote this convergence as $X_n \xrightarrow{d} X$.

    \item We say $X_n$ \textbf{converges in probability} to $X$ if for any $\epsilon > 0$,
          \begin{equation}
              \lim_{n \rightarrow \infty} P(d(X_n, X) > \epsilon) = 0
          \end{equation}
          where $d(\cdot, \cdot)$ is a distance metric between like Euclidean distance. We denote this convergence as $X_n \xrightarrow{P} X$.

    \item We say $X_n$ \textbf{converges almost surely} to $X$ if
          \begin{equation}
              P\left(\lim_{n \rightarrow \infty} d(X_n, X) = 0  \right) = 1
          \end{equation}
          we also denote this convergence as $X_n \xrightarrow{a.s.} X$. This is considered as a stronger form of convergence than the two modes above.
\end{itemize}

\begin{lemma}
    \label{lemma:portmanteau}
    \textbf{(Portmanteau)} For any sequence of random variables $X_n$ and $X$ the following statements are equivalent.
    \begin{itemize}
        \item[(i)] $P(X_n \leq x) \to P(X \leq x)$ for all continuity points of $x \mapsto P(X \leq x)$;
        \item[(ii)] $\E f(X_n) \to \E f(X)$ for all bounded, continuous functions $f$;
        \item[(iii)] $\E f(X_n) \to \E f(X)$ for all bounded, Lipschitz functions $f$;
        \item[(iv)] $\liminf \E f(X_n) \geq \E f(X)$ for all nonnegative, continuous functions $f$;
        \item[(v)] $\liminf P(X_n \in G) \geq P(X \in G)$ for every open set $G$;
        \item[(vi)] $\limsup P(X_n \in F) \leq P(X \in F)$ for every closed set $F$;
        \item[(vii)] $P(X_n \in B) \to P(X \in B)$ for all Borel sets $B$ with $P(X \in \delta B) = 0$, where $\delta B = \overline{B} - \overset{\circ}{B}$ is the boundary of $B$.
    \end{itemize}
\end{lemma}

The textbook has proven all the equivalence except for $(ii) \Leftrightarrow (iv)$, which we give the proof below together with other proof supplements.

\begin{proof}[Proof supplements of Lemma 1]
    We first supplement $(i) \Rightarrow (ii)$ by proving that a continuous function on a compact set is uniformly continuous. Let $f$ be a continuous function on a compact domain $K$. By contradiction, if $f$ is not uniformly continuous, let $\epsilon > 0$, then for every $n \in \mathbb{N}$, there exists $x_n, y_n \in K$ such that $|x_n - y_n| < 1/n$ but $|f(x_n) - f(y_n)| \geq \epsilon$. Since $K$ is compact, we can extract a convergent subsequence $x_{n_k} \to x$ and $y_{n_k} \to y$. Then by continuity of $f$, we have $f(x) = \lim_{k \to \infty} f(x_{n_k}) = \lim_{k \to \infty} f(y_{n_k}) = f(y)$, which contradicts the assumption that $|f(x) - f(y)| \geq \epsilon$.

    Next we clarify $(iii) \Rightarrow (v)$ by explaining the Lipschitz approximation to the indicator function on open set $G$. Consider the sequence of Lipschitz functions $f_m$ defined as
    \begin{equation}
        f_m(x) = \begin{cases}
            1                 & \text{if } d(x, G^c) \geq 1/m \\
            m \cdot d(x, G^c) & \text{if } d(x, G^c) < 1/m
        \end{cases}
    \end{equation}
    then $f_m$ is apparently Lipschitz with Lipschitz constant $m$. Since $f_m \uparrow \mathbf{1}_G$ pointwise, by $(iii)$ we have for every $m$
    \begin{equation}
        \liminf_{n\to\infty} P(X_n \in G) \geq \liminf _{n\to\infty}\E f_m(X_n) = \E f_m(X)
    \end{equation}
    and the RHS $\to P(X \in G)$ as $m \to \infty$.

    Finally, we prove $(ii) \Leftrightarrow (iv)$. Starting from the assumption $(ii)$. Let $f_m$ be a sequence of bounded, continuous functions such that $f_m \uparrow f$. We can expcliditly write $f_m$ as $f_m = \min(f, m)$. For each fixed $m$, we have
    \begin{equation}
        \liminf_{n\to\infty} \E f(X_n) \geq \liminf_{n\to\infty} \E f_m(X_n) = \E f_m(X)
    \end{equation}
    where the last equality is by $(ii)$. As $m \to \infty$, the RHS increases to $\E f(X)$ by the monotone convergence theorem.

    For the reverse direction, let $m$ and $M$ be the lower and upper bounds of $f$, and define functions $g = f + m$ and $h = M - f$. Apparently both $g$ and $h$ are nonnegative, continuous functions. Then we have
    \begin{equation}
        \liminf_{n\to\infty} \E g(X_n) \geq \E g(X) \quad \text{and} \quad \liminf_{n\to\infty} \E h(X_n) \geq \E h(X)
    \end{equation}
    From the first inequality, we have $\liminf_{n\to\infty} \E f(X_n) \geq \E f(X)$. Similarly, from the second inequality, we have $\limsup_{n\to\infty} \E f(X_n) \leq \E f(X)$. Combining the two inequalities, we have $\liminf_{n\to\infty} \E f(X_n) = \E f(X)$.
\end{proof}

The next theorem is about how continuous mapping preserves all three modes of convergence mentioned above.

\begin{theorem}
    \label{thm:cont_map_thm}
    \textbf{(Continuous Mapping Theorem).} Let $g: \mathbb{R}^d \to \mathbb{R}^k$ be continuous at every point of a set \( C \) such that \( P(X \in C) = 1 \)
    \begin{enumerate}
        \item[(i)] If $X_n \xrightarrow{d} X$, then $g(X_n) \xrightarrow{d} g(X)$.
        \item[(ii)] If $X_n \xrightarrow{P} X$, then $g(X_n) \xrightarrow{P} g(X)$.
        \item[(iii)] If $X_n \xrightarrow{a.s.} X$, then $g(X_n) \xrightarrow{a.s.} g(X)$.
    \end{enumerate}
\end{theorem}

\begin{proof}[Proof supplment of Theorem \ref*{thm:cont_map_thm}]
    For $(i)$, the easier way than the textbook is to realize that the preimage of a closed set under a continuous function is closed. Then we can utlize $(vi)$ in Lemma \ref{lemma:portmanteau} to prove $P(g(X_n) \in F) \to P(g(X) \in F)$ for every closed $F$.

    The $(iii)$ is trivial to prove: if the event $\lim_{n\to\infty}d(X_n, X) = 0$ is true, by continuity of $g$ we have $\lim_{n\to\infty}d(g(X_n), g(X)) = 0$ as well. Therefore:
    \begin{equation}
        P\left(\lim_{n\to\infty}d(g(X_n), g(X)) = 0\right) \geq P\left(\lim_{n\to\infty}d(X_n, X) = 0\right) = 1
    \end{equation}
\end{proof}

% ------------------------------------------------------------------------------
For a sequence of random variables $\left\{ X_n \right\}$, the concept of \textbf{uniformly tightness} (also known as \textbf{bounded in probability}) states that for every $\epsilon > 0$, there exists a constant $M$ such that
\begin{equation}
    \sup_{n} P(|X_n| > M) < \epsilon
\end{equation}
\textbf{Prohorov's theorem} then states the equivalence between uniformly tightness and convergence in distribution.
\begin{theorem}\label{thm:prohorov}
    Let $\left\{ X_n \right\}$ be a sequence of random variables in $\mathbb{R}^k$.
    \begin{itemize}
        \item[(i)] If $X_n \xrightarrow{d} X$ for some $X$, then $\{X_n : n \in \mathbb{N}\}$ is uniformly tight;
        \item[(ii)] If $X_n$ is uniformly tight, then there exists a subsequence with $X_{n_j} \xrightarrow{d} X$ as $j \to \infty$, for some $X$.
    \end{itemize}
\end{theorem}
\begin{proof}[Note on the proof for $(i)$]
    There is a very trivial fact used in proving $(i)$, which we will elaborate here. We want to show that for any well-defined random variable $X$ and $\epsilon > 0$, there exists $M$ such that $P(|X| > M) < \epsilon$. This is equivalent to showing that $P(|X| \leq M) \geq 1 - \epsilon$. Since $P(|X| \leq M)$ is a monotonically increasing function of $M$, we can take $M$ to be the smallest value such that $P(|X| \leq M) \geq 1 - \epsilon$. We can also prove it using Markov's inequality on $|X|$.

    For the proof on $(ii)$, it relies on the following \textbf{Helley's Lemma}.
\end{proof}
\begin{lemma}[\textbf{Helley's Lemma}]
    Each given sequence $F_n$ of cumulative distribution functions on $\mathbb{R}^k$ possesses a subsequence $F_{n_j}$ with the property that $F_{n_j}(x) \to F(x)$ at each continuity point $x$ of a possibly \textbf{defective distribution function} $F$.
    \vspace*{0.2em}

    A \textbf{defective distribution function} is one that shares the same properties as a distribution function, except that its limits at infinities might be between 0 and 1.
\end{lemma}
\begin{proof}[Proof supplement] we skip over everything up to the diagonalization argument. Now we have a subsequence \( \left\{ F_{n_j} \right\}\) such that \(F_{n_j}(q_i) \to G(q_i)\) for each $q \in \mathbb{Q}$ (here for simplicity we deal with 1-dimensional case). We define
    \begin{equation}
        F(x) = \inf_{q > x} G(q)
    \end{equation}
    here we mainly supplement the ``monotonicity'' argument. If $x$ is a coninuity point of $F$, then we can always find rationals $q < x < q'$ such that $G(q) < F(x) < G(q')$ and $G(q') - G(q) < \epsilon$ for any $\epsilon > 0$. Now since $F_{n_j}(q) \to G(q)$ and same for $q'$, we have
    \begin{equation}
        G(q') = \liminf F_{n_j}(q') \geq \liminf F_{n_j}(x) \geq \liminf F_{n_j}(q) = G(q)
    \end{equation}
    which directly tells us that $\|\liminf F_{n_j}(x) - F(x)\| < \epsilon$. Similarly showing the $\limsup$ part gives us the desired result.
\end{proof}
\subsection{Connection Between Different Modes of Convergence}
The following theorem gives the connection between different modes of convergence.
\begin{theorem}
    Let $X_n, X$ and $Y_n$ be random vectors. Then
    \begin{itemize}
        \item[(i)] $X_n \xrightarrow{a.s.} X$ implies $X_n \xrightarrow{P} X$;
        \item[(ii)] $X_n \xrightarrow{P} X$ implies $X_n \xrightarrow{d} X$;
        \item[(iii)] $X_n \xrightarrow{P} c$ for a constant $c$ if and only if $X_n \xrightarrow{d} c$;
        \item[(iv)] if $X_n \xrightarrow{d} X$ and $d(X_n, Y_n) \xrightarrow{P} 0$, then $Y_n \xrightarrow{d} X$;
        \item[(v)] if $X_n \xrightarrow{d} X$ and $Y_n \xrightarrow{P} c$ for a constant $c$, then $(X_n, Y_n) \xrightarrow{d} (X, c)$;
        \item[(vi)] if $X_n \xrightarrow{P} X$ and $Y_n \xrightarrow{P} Y$, then $(X_n, Y_n) \xrightarrow{P} (X, Y)$.
    \end{itemize}
\end{theorem}
A quick conclusion is that convergence in distribution is the weakest form of convergence, while almost sure convergence is the strongest.
\begin{proof}[Proof supplement]\label{thm:connection}
    The proof for $(i)$ is trivial: if $X_n \xrightarrow{a.s.} X$, then for any $\epsilon > 0$ if we define $A_n = \left\{ k \geq n : d(X_k, X) > \epsilon \right\}$, we know that $A_n \downarrow \emptyset$ as $n \to \infty$. Therefore
    \begin{equation}
        \lim_{n\to\infty} P(d(X_n, X) > \epsilon) \leq \lim_{n\to\infty} P(A_n) = 0
    \end{equation}
    it is worth noting that the last equality is by continuity of probability measure.

    For $(ii)$ we supplement a direct proof by showing that convergence in probability leads to $\E f(X_n) \to \E f(X)$ by Lemma \ref{lemma:portmanteau} for all bounded, continuous functions $f$. Firstly recall that by continuous mapping theorem, if $f$ is continuous, then $f(X_n) \xrightarrow{P} f(X)$, i.e.
    \begin{equation}
        \lim_{n\to\infty} P(|f(X_n) - f(X)| > \epsilon) = 0
    \end{equation}
    for any fixed $\epsilon > 0$. As a result, we have
    \begin{equation}
        \lim_{n\to\infty} \E |f(X_n) - f(X)| \leq \epsilon
    \end{equation}
    since $\epsilon$ is arbitrary, we have $\E |f(X_n) - f(X)| \to 0$. This is equivalent to $\E f(X_n) \to \E f(X)$.
\end{proof}
The $(v)$ in Theorem $\ref{thm:connection}$ is especially useful in proving the \textbf{Slutsky's lemma}.
\begin{lemma}[Slutsky's Lemma]
    Let $X_n, X$ and $Y_n$ be random variables. If $X_n \xrightarrow{d} X$ and $Y_n \xrightarrow{d} c$ for a constant $c$, then
    \begin{enumerate}
        \item[(i)] $X_n + Y_n \xrightarrow{d} X + c$
        \item[(ii)] $X_n Y_n \xrightarrow{d} X \cdot c$
        \item[(iii)] $X_n / Y_n \xrightarrow{d} X / c$ if $c \neq 0$.
    \end{enumerate}
\end{lemma}
In each of the cases above, we leverage points $(iii), (v)$ in $\ref{thm:connection}$ first, then define a continuous mapping $f(X_n, Y_n)$ to enable continuous mapping.

Finally, if the distribution function of the limit $X$ is continuous, then convergence in distribution implies uniform convergence of $P(X_n < x) \to P(X < x)$ for every $x$.
\begin{lemma}
    Let $X_n$ and $X$ be random variables and $X$ has a continuous distribution function. If $X_n \xrightarrow{d} X$, then $P(X_n < x) \to P(X < x)$ uniformly in $x$.
\end{lemma}
\begin{proof}[Proof for $d = 2$]
    Here we supplement a proof for the 2-dimensional case. Let $F$ and $F_n$ denotes the distribution function of $X$ and $X_n$ respectively. By continuity of $F$ we can find $k$ points $-\infty = a_1 < a_2 < \cdots < a_{k} = \infty$ on the first coordinate and another $k$ points $-\infty = b_1 < b_2 < \cdots < b_{k} = \infty$ on the second coordinate such that $F(a_i, b_j) = (ij)/k^2$.

    Now consider any point $(a, b) \in \mathbb{R}^2$, we can find $i, j$ such that $a \in [a_i, a_{i+1}]$ and $b \in [b_j, b_{j+1}]$. We thus have the inequalities
    \begin{align*}
        F_n(a, b) - F(a, b) & \leq F_n(a_{i+1}, b_{j+1}) - F(a_i, b_j) \\ &= F_n(a_{i+1}, b_{j+1}) - F(a_{i+1}, b_{j+1}) + (i + j + 1)/k^2
        \\ &\geq F_n(a_{i}, b_{j}) - F(a_{i+1}, b_{j+1})
        \\ &= F_n(a_{i}, b_{j}) - F(a_{i}, b_{j}) - (i + j + 1)/k^2
    \end{align*}
    the rest is the same as the 1-dimensional case since $F_n \to F$ and $(i + j + 1)/k^2 \leq (2k + 1)/k^2$ will vanish as $k \to \infty$.
\end{proof}

\subsection{Stochastic Big-O and Little-o Notations}
\begin{definition}
    The $O_p$ (Big-O) notation denotes how a sequence of random variables are bounded in probability. We say $X_n = O_p(a_n)$ if for every $\epsilon > 0$, there exists finite $M, N > 0$ such that
    \begin{equation}
        \sup_{m \geq N} P\left( \left|\frac{X_m}{a_m}\right| > M \right) < \epsilon
    \end{equation}
\end{definition}
\begin{definition}
    The $o_p$ (Little-o) notation denotes how a sequence of random variables converges to 0 in probability. We say $X_n = o_p(a_n)$ if for every $\epsilon > 0$, we have
    \begin{equation}
        \lim_{n \to \infty}P\left( \left|\frac{X_n}{a_n}\right| > \epsilon \right) = 0
    \end{equation}
    which is essentially $\frac{X_n}{a_n} \xrightarrow{P} 0$.
\end{definition}
A few important results can be shown using the definitions. The commonly used ones are:
\begin{enumerate}
    \item If $X_n = O_p(1)$ and $Y_n = O_p(1)$, then $X_n + Y_n = O_p(1)$;
    \item If $X_n = O_p(1)$ and $Y_n = o_p(1)$, then $X_nY_n = o_p(1)$;
    \item If $X_n = o_p(1)$ and $Y_n = o_p(1)$, then $X_n + Y_n = o_p(1)$.
    \item If $X_n = O_p(1)$ and $Y_n = O_p(1)$, then $X_nY_n = O_p(1)$.
\end{enumerate}
For the fourth point, we can easily find $M_1, M_2, N_1, N_2$ such that
\begin{equation}
    P\left( \left|X_n\right| > M_1 \right) < \epsilon \quad \text{and} \quad P\left( \left|Y_n\right| > M_2 \right) < \epsilon
\end{equation}
for $n \geq \max{N_1, N_2}$. Then we have
\begin{equation}
    P\left( \left|X_nY_n\right| > M_1M_2 \right) \leq P\left( \left|X_n\right| > M_1 \right) \cdot P\left( \left|Y_n\right| > M_2 \right) < \epsilon^2
\end{equation}
which easily implies $X_nY_n = O_p(1)$ since $\epsilon$ is arbitrary.
\newpage

% section 5

\setcounter{section}{4}
\section{M- and Z- Estimators}
\subsection{Definitions and Notations}
M-estimators are a class of estimators that are defined by maximizing an average of some functions of the data. Let $X_1, ..., X_n$ be i.i.d samples from a distribution with parameter of interest $\theta$. We consider a known function $m_\theta:\mathcal{X} \to \mathbb{R}$, and define the objective
\begin{equation}
    \theta \mapsto M_n(\theta) = \frac{1}{n} \sum_{i=1}^{n} m_\theta(X_i)
\end{equation} 
\begin{definition}
    An estimator that maximizes $M_n(\theta)$ over all $\theta \in \Theta$ is called an \textbf{M-estimator}.
\end{definition}
The solution of $\theta^* = \argmax_{\theta \in \Theta} M_n(\theta)$ is usually carried out by differentiating $M_n(\theta)$ and setting it to 0. This gives a system of equations in the form of 
\begin{equation}\label{eq:m-est}
    \Psi_n(\theta) = \sum_{i=1}^{n} \psi_\theta(X_i) = 0
\end{equation}
which are also known as \textbf{estimating equations}. Note that if $\theta \in \mathbb{R}^k$, then $\psi_\theta = (\psi_{\theta, 1}, ..., \psi_{\theta, k})$ is a vector-valued function and \ref{eq:m-est} is a short-hand notation for
\begin{equation}
    \sum_{i=1}^{n} \psi_{\theta, j}(X_i) = 0, \quad  j = 1, ..., k
\end{equation}
\begin{definition}
    An estimator that satisfies the estimating equations is called a \textbf{Z-estimator}, but it is often called a \textbf{M-estimator} as well.
\end{definition}
\begin{example}
    For the well-known \textbf{maximium likelihood estimator} (MLE), we seek the parameter $\theta$ that maximizes the likelihood function $L(\theta) = \prod_{i=1}^{n} p_\theta(X_i)$, which is equivalent to maximizing the log-likelihood function $\ell(\theta) = \sum_{i=1}^{n} \log p_\theta(X_i)$. Putting it under the above framework, we have 
    \begin{equation*}
        m_\theta(x) = \log p_\theta(x) \quad \text{and} \quad \psi_\theta(x) = \nabla_\theta \log p_\theta(x)
    \end{equation*}
\end{example}
Next we summarize some useful notations surrounding the expectations and empirical (distribution) averages of the samples.

Let $P$ be the law of i.i.d. samples $X_1, ..., X_n$. For a measurable function $f: \mathcal{X} \to \mathbb{R}$, we denote $Pf := \E f(X) = \int f\, dP$. We let $\mathbb{P}_n$ be the empirical measure of the samples, i.e. $\mathbb{P}_n = n^{-1} \sum_{i=1}^{n} \delta_{X_i}$, where $\delta_x$ is the Dirac measure at $x$. We also denote $\mathbb{P}_n f = \int f\, d\mathbb{P}_n = n^{-1} \sum_{i=1}^{n} f(X_i)$. The above equations then can be rewritten as 
\begin{equation}
    M_n(\theta) = \mathbb{P}_n m_\theta \quad \text{and} \quad \Psi_n(\theta) = \mathbb{P}_n \psi_\theta
\end{equation}
Finally, we define the empirical process $\mathbb{G}_n f$ as the centered sums $n^{-1/2} \sum_{i=1}^n \left( f(X_i) - Pf \right)$.

\subsection{Consistency}
When we design an estimator $\hat \theta_n$, it is desirable that $\hat \theta_n$ converges in probability to the ``true parameter'' of interest $\theta$ as $n \to \infty$. For M-estimators, the definition is
\begin{definition}
    Let $\hat \theta_n$ be the M-estimator for samples $X_1, ..., X_n$ so that 
    \begin{equation}
        \hat \theta_n = \argmax_{\theta \in \Theta} M_n(\theta) = \argmax_{\theta \in \Theta} \mathbb{P}_n m_\theta
    \end{equation}
    let $P$ be the law of the samples. The parameter (or ``estimand'') of interest $\theta$ satisfies
    \begin{equation}
        \theta = \argmax_{\theta \in \Theta} M_\theta := \argmax_{\theta \in \Theta} P m_\theta
    \end{equation}
    then if for all values of $\theta$, $\hat \theta_n \xrightarrow{P} \theta$, we say that $\hat \theta_n$ is \textbf{asymptotically consistent}.
\end{definition} 
Despite the definition above, it is \textbf{not guaranteed} that the maximizer for $M_n(\theta)$ will converge to the maximizer of $M_\theta$, provided that the latter exists and is unique. Proving this is the focus of this section. Also note that it is not hard (by Law of Large Numbers and continuous mapping) to find a deterministic function $M'(\theta)$ such that
\begin{equation}
    \label{eq:point_conv}
    M_n(\theta) \xrightarrow{P} M'(\theta) \quad \text{for all } \theta \in \Theta
\end{equation}

The following theorem estiablishes the conditions for the consistency of M-estimators.
\begin{theorem}[Consistency of M-estimators]\label{thm:mle_consistency}
    Let $M_n$ be random functions and let $M$ be a fixed function of $\theta$ such that for every $\epsilon > 0$
    \begin{equation}
        \sup_{\theta \in \Theta} | M_n(\theta) - M(\theta) | \overset{P}{\rightarrow} 0,
    \end{equation}
    \begin{equation}
        \sup_{\theta : d(\theta, \theta_0) \geq \epsilon} M(\theta) < M(\theta_0).
    \end{equation}
    Then any sequence of estimators $\hat{\theta}_n$ with $M_n(\hat{\theta}_n) \geq M_n(\theta_0) - o_P(1)$ converges in probability to $\theta_0$.
\end{theorem}
The key insights into the two conditions are:
\begin{itemize}
    \item The first condition refers to uniform convergence of $M_n$ to $M$ in probability, which is stronger than the pointwise convergence in \ref{eq:point_conv}.
    \item The second condition indicates that $\theta_0$ is a \textit{well-separated} maximizer of $M$. It can be shown that this is equivalent to $\theta_0$ being the \textit{unique maximizer} of $M$.
    \item The requirement for $\hat \theta_n$ is that it \textit{nearly maximizes} $M_n$, which is a weaker condition than requiring it to be the maximizer.
\end{itemize}
\begin{proof}[Proof supplement]
    the following claim is not explained clearly in the VDV book: given that $M_n(\hat{\theta}_n) \geq M_n(\theta_0) - o_P(1)$ and $M_n(\theta_0) \overset{P}{\rightarrow} M(\theta_0)$, we have $M_n(\hat{\theta}_n) \geq M(\theta_0) - o_P(1)$.

    We first notice that if we rewrite $a_n = M(\theta_0) - M_n(\theta_0)$, then $a_n \overset{P}{\rightarrow} 0$, so $a_n = o_P(1)$. We can re-express $M_n(\theta_0) = M(\theta_0) - o_P(1)$, then 
    \begin{equation}
        M_n(\hat{\theta}_n) \geq M_n(\theta_0) - o_P(1) \geq M(\theta_0) - o_P(1) - o_P(1) = M(\theta_0) - o_P(1)
    \end{equation}
    where the last equality follows from the fact that $o_P(1) + o_P(1) = o_P(1)$.
\end{proof}
Theorem \ref{thm:mle_consistency} for M-estimators can be easily applied to Z-estimators, which are essentially the zeros of the criterion functions $\theta \mapsto \Psi_n(\theta)$. Note that any zero of $\Psi_n(\theta)$ is also a maximizer (i.e. M-estimator) of the criterion $\theta \mapsto -\Vert\Psi_n(\theta)\Vert$. The following theorem establishes similar conditions for the consistency of Z-estimators.
\begin{theorem}[Consistency of Z-estimators]\label{thm:z_consistency}
    Let $\Psi_n$ be random vector-valued functions and let $\Psi$ be a fixed vector-valued function of $\theta$ such that for every $\epsilon > 0$
    \begin{equation}
    \sup_{\theta \in \Theta} \| \Psi_n(\theta) - \Psi(\theta) \| \overset{P}{\rightarrow} 0,
    \end{equation}
    \begin{equation}
    \inf_{\theta : d(\theta, \theta_0) \geq \epsilon} \| \Psi(\theta) \| > 0 = \| \Psi(\theta_0) \|.
    \end{equation}
    Then any sequence of estimators $\hat{\theta}_n$ such that $\Psi_n(\hat{\theta}_n) = o_P(1)$ converges in probability to $\theta_0$.
\end{theorem}
The proof is trivial and can be done by following the same steps as in Theorem \ref{thm:mle_consistency}. It should be instead noted that both of the theorems above require some sort of uniform convergence of the criterion functions, which is a strong condition. The following lemma shows one way to replace uniformity with other conditions.
\begin{lemma}
    Let $\Theta$ be a subset of the real line and let $\Psi_n$ be random functions and $\Psi$ a fixed function of $\theta$ such that $\Psi_n(\theta) \overset{P}{\rightarrow} \Psi(\theta)$ in probability for every $\theta$. Assume that each map $\theta \mapsto \Psi_n(\theta)$ is continuous and has exactly one zero $\hat{\theta}_n$, or is nondecreasing with $\Psi_n(\hat{\theta}_n) = o_P(1)$. Let $\theta_0$ be a point such that $\Psi(\theta_0 - \epsilon) < 0 < \Psi(\theta_0 + \epsilon)$ for every $\epsilon > 0$. Then $\hat{\theta}_n \overset{P}{\rightarrow} \theta_0$.
\end{lemma}
\begin{proof}[Proof supplement]
    Some simple derivations to make the VDV book argument more complete.
    Firstly, if the map $\theta \mapsto \Psi_n(\theta)$ is continuous and has exactly one zero $\hat{\theta}_n$, then
    \begin{equation}
        0 \in (\Psi_n(\theta_0  - \epsilon), \Psi_n(\theta_0 + \epsilon)) \quad \Longrightarrow \quad \hat{\theta}_n \in (\theta_0  - \epsilon, \theta_0  + \epsilon)
    \end{equation}
    since continuity (intermediate value theorem) requires some point $\theta'$ in the interval that makes $\Psi_n(\theta') = 0$, and $\theta'$ must be $\hat{\theta}_n$ by uniqueness. 

    We also provide a slightly clearer proof (sketch) for the nondecreasing case. For any $\epsilon, \eta > 0$, if $\Psi_n(\theta_0 - \epsilon) < -\eta$ and $\hat{\theta}_n \leq \theta_0 - \epsilon$, then by nondecreasing property we have $\Psi_n(\hat{\theta}_n) < \epsilon$. Thus
    \begin{equation}
        P\left( \Psi_n(\theta_0 - \epsilon) < -\eta \,\cap\, \hat{\theta}_n \leq \theta_0 - \epsilon \right) \leq P\left( \Psi_n(\hat{\theta}_n) < \epsilon \right) \to 0
    \end{equation}
    as $n \to \infty$ since $\Psi_n(\hat{\theta}_n) = o_P(1)$. Equivalently, we have
    \begin{equation}
        P\left( \Psi_n(\theta_0 - \epsilon) < -\eta \,\cap\, \hat{\theta}_n > \theta_0 - \epsilon \right) \to P\left( \Psi_n(\theta_0 - \epsilon) < -\eta \right)
    \end{equation}
    now, since $\eta$ is arbitrary, we can simply set $\eta = -\Psi(\theta_0 - \epsilon) / 2$. Since $\Psi_n(\theta_0 - \epsilon) \to \Psi(\theta_0 - \epsilon) < 0$, we have the RHS above converges to 1 and LHS to $P( \hat{\theta}_n > \theta_0 - \epsilon)$. Repeating this for the other direction, i.e. the case when $\Psi_n(\theta_0 + \epsilon) > \eta$ and $\hat{\theta}_n \geq \theta_0 + \epsilon$, we can show that
    \begin{equation}
        P\left( \theta_0 + \epsilon > \hat{\theta}_n  > \theta_0 - \epsilon \right) \to 0
    \end{equation}
    which implies $\hat{\theta}_n \overset{P}{\rightarrow} \theta_0$.
\end{proof}
\newpage
% ------------------------------------------------------------------------------
% Chapter 6
% ------------------------------------------------------------------------------
\section{Contiguity}
\begin{note}
    In probability theory, two sequences of probability measures are said to be \textbf{contiguous} if asymptotically they share the same support. Thus the notion of \textbf{contiguity} extends the concept of absolute continuity to the sequences of measures.

    The concept was originally introduced by \textbf{Le Cam} (1960) as part of his foundational contribution to the development of asymptotic theory in mathematical statistics. This section blends in VDV's textbook and Peter Barlett's STAT 210B lecture notes\footnotemark.
\end{note}
\footnotetext{https://www.stat.berkeley.edu/~bartlett/courses/2013spring-stat210b/notes/21notes.pdf}

\subsection{Likelihood Ratios (Radon-Nikodym Derivatives)}
We start by considering two probability measures $P$ and $Q$ on a common measurable space $(\Omega, \mathcal{A})$. The definitions of \textbf{absolutely continuous} and \textbf{orthogonal} (singular) for measures are as follows:
\begin{definition}
    Measure $P$ is \textbf{absolutely continuous} with respect to $Q$ if $Q(A) = 0$ implies $P(A) = 0$ for all $A \in \mathcal{A}$. We denote this as $P \ll Q$.
\end{definition}
\begin{definition}
    Measure $P$ is \textbf{orthogonal} (singular) with respect to $Q$ if there exists a set $A \in \mathcal{A}$ such that $Q(A) = 0$ and $P(A^c) = 0$. We denote this as $P \perp Q$.
\end{definition}
With these definitions, we can start look into the densities. Let $p$ and $q$ be the densities of $P$ and $Q$ with respect to a common measure $\mu$ (Lebesgue measure). Consider the set $\Omega_P = \left\{ \omega : p(\omega) > 0 \right\}$ and $\Omega_Q = \left\{ \omega : q(\omega) > 0 \right\}$. Then by defintion, we can write measure $Q$ as the sum $Q = Q^a + Q^\perp$ where
\begin{equation}
    Q^a(A) = Q(A\cap \Omega_p) \quad \text{and} \quad Q^\perp(A) = Q(A\cap \Omega_p^c)
\end{equation}
for any measurable set $A$. This is also called the \textbf{Lebesgue decomposition} of $Q$ with respect to $P$. Using these definitions, we have the following lemma:
\begin{lemma}\label{lemma:contiguity}
    Let \(P\) and \(Q\) be probability measures with densities \(p\) and \(q\) with respect to a measure \(\mu\).
    \begin{enumerate}
        \item[(i)] \(Q = Q^a + Q^\perp\), \(Q^a \ll P\), \(Q^\perp \perp P\).
        \item[(ii)] \(Q^a(A) = \int_A \left(q/p\right) dP\) for every measurable set \(A\).
        \item[(iii)] \(Q \ll P \:\Longleftrightarrow\: Q(\{p = 0\}) = 0 \:\Longleftrightarrow\: \int \left(q/p\right) dP = 1\).
    \end{enumerate}
\end{lemma}
\begin{proof}[Proof supplement]
    Here's a clearer proof for $Q^a \ll P$. Consider any $\mathcal{O} \in \mathcal{A}$ such that $P(\mathcal{O})$. Thus we have $\mu(\mathcal{O}\cap\Omega_P) = \mu(\{x \in \mathcal{O}: p(x) > 0\}) = 0$. Thus $Q^\perp(\mathcal{O}) = 0$ since $\mu$ is the dominating measure.
\end{proof}
The intuition behind $(iii)$ in Lemma \ref{lemma:contiguity} is immense. For any two measures $P$ and $Q$, very likely they are neither absolutely continuous nor orthogonal. However, there is always a way to ``generate'' a new measure $Q^a$ from $Q$ that is absolutely continuous with respect to $P$ (and vice versa). $Q^a$ might not be a valid probability measure, but when it does, we know that $Q$ is absolutely continuous with respect to $P$.

In statistics, the Radon-Nikodym derivative $q/p$ is the density of $Q^a$ with respect to $P$, as illustrated in \ref{lemma:contiguity} $(ii)$. However, people often denotes:
\begin{equation}\label{eq:p-eq-def}
    \frac{dQ}{dP} := \frac{dQ^a}{dP} = \frac{q}{p} \quad P - a.s.
\end{equation}
which is only $P$-almost surely unique, and we can feel free to define the quotient for $p = 0$. In general, for nonnegative measurable function $f$ and arbitrary measures $P$ and $Q$:
\begin{equation}\label{eq:p-eq-ineq}
    \int f \, dQ \geq \int_{p>0} f q \, d\mu = \int_{p>0} f \frac{q}{p} p \, d\mu = \int f \frac{dQ}{dP} \, dP
\end{equation}

\subsection{Contiguity of Sequences of Measures}
\begin{definition}
    The sequence of random variables $Q_n$ is \textbf{contiguous} to the sequence $P_n$ if $P_n(A_n) \to 0$ implies $Q_n(A_n) \to 0$ for all sequences of measurable sets $A_n$. We denote this as $P_n \triangleleft Q_n$. $P_n$ and $Q_n$ are \textbf{mutually contiguous} if $P_n \triangleleft Q_n$ and $Q_n \triangleleft P_n$.
\end{definition}
$dQ_n/dP_n$ and $dP_n/dQ_n$ defined from \ref{eq:p-eq-def} are nonnegative, and results from \ref{eq:p-eq-ineq} directly shows
\begin{equation}
    \E_{P_n} \frac{dQ_n}{dP_n} \leq 1 \quad \text{and} \quad \E_{Q_n} \frac{dP_n}{dQ_n} \leq 1
\end{equation}
Thus, Prohorov's theorem (Theorem \ref{thm:prohorov}) tells us that both $dQ_n/dP_n$ and $dP_n/dQ_n$ have a weakly convergent subsequence\footnote{by Radon-Nikodym it is easy to see that these quotients are functions $\Omega \to [0, \infty)$, so we can treat them as random variables.}. The key insight is that the contiguity relationship between $P_n$ and $Q_n$ can be understood by investigating the asymptotic behavior of these subsequences.
\begin{lemma}[Le Cam's First Lemma]\label{lemma:lecam1}
    Let \(P_n\) and \(Q_n\) be sequences of probability measures on measurable spaces \((\Omega_n, \mathcal{A}_n)\). Then the following statements are equivalent:
    \begin{enumerate}
        \item[(i)] \(Q_n \triangleleft P_n\).
        \item[(ii)] If \(dP_n/dQ_n \overset{Q_n}{\rightsquigarrow} U\) along a subsequence, then \(\text{P}(U > 0) = 1\).
        \item[(iii)] If \(dQ_n/dP_n \overset{P_n}{\rightsquigarrow} V\) along a subsequence, then \(\text{EV} = 1\).
        \item[(iv)] For any statistics \(T_n : \Omega_n \mapsto \mathbb{R}^k\): If \(T_n \overset{P_n}{\rightsquigarrow} 0\), then \(T_n \overset{Q_n}{\rightsquigarrow} 0\).
    \end{enumerate}
\end{lemma}
Note that we write $A_n \overset{Q_n}{\rightsquigarrow} B$ if $A_n$ converges in distribution to $B$ along a subsequence under measure $Q_n$, i.e. $Q_n(A_n < x) \to P(B < x)$ for all continuity points.

Le Cam's first lemma is a direct and asymptotic analogy of the results in Lemma \ref{lemma:contiguity}. A cleaner proof than the VDV textbook is given in the review by Li and Babu (2019)\footnote{https://par.nsf.gov/servlets/purl/10202267}. Here's one quote from the review on the difference beteween contiguity and absolute continuity:
\begin{quote}
    Despite their conceptual similarity, contiguity and absolute continuity are technically very different: the former characterizes the collective behavior of a pair of sequences of probability measures; the latter that of a pair of probability measures.
\end{quote}
A direct result from Lemma \ref{lemma:lecam1} is what we call the \textbf{Le Cam's third lemma}, which solves the problem of obtaining a $Q_n$-limit law from a $P_n$-limit one.
\begin{lemma}[Le Cam's Third Lemma]
    Let \(P_n\) and \(Q_n\) be sequences of probability measures on measurable spaces \((\Omega_n, \mathcal{A}_n)\), and let $X_n: \Omega_n \to \mathbb{R}^k$ be a sequence of random vectors. If \(Q_n \triangleleft P_n\) and 
    \begin{equation}
        \left( X_n, \frac{dQ_n}{dP_n} \right) \overset{P_n}{\rightsquigarrow} (X, V)
    \end{equation}
    Then $L(B) = \E [\mathbf{1}_B(X)V]$ defines the limiting probability measure so that $X_n \overset{Q_n}{\rightsquigarrow} L$.
\end{lemma}  
\begin{proof}[Proof supplement]
    By definition we have $L(\Omega) = \E[V] = 1$, where the last equality is by $(iii)$ Lemma \ref{lemma:lecam1}. This makes sure that $L$ is a valid probability measure. For every measurable step function $f$, we have
    \begin{equation}
        \int f \, dL = L(\left\{ f = 1 \right\})= \E[f(X)V]
    \end{equation}
    and we can extend this relationship to more complicated functions (simple function, nonnegative function, etc.). Now for any continuous and nonnegative function $f$, we define $g: (x, v) \to f(x)v$ on $\mathbb{R}^k \times [0, \infty)$. Then $g$ is continuous and nonngative as well. Thus, by $(iv)$ in Lemma \ref{lemma:portmanteau}, we have 
    \begin{align}
        \liminf_{n\to\infty} \E_{P_n}\, g\left( X_n, \frac{dQ_n}{dP_n} \right) &=  \liminf_{n\to\infty} \int f(X_n)\frac{dQ_n}{dP_n} dP_n\\ 
        & = \color{red} \liminf_{n\to\infty} \int f(X_n) \,dQ_n\\ 
        & \geq \int f(X) V dP_{X, V} \\
        & = \color{Plum} \int f(x) \, dL(x)
    \end{align}
    where the {\color{red}red} $\geq$ {\color{Plum}plum} inequality gives us that $X_n \overset{Q_n}{\rightsquigarrow} L$ by Lemma \ref{lemma:portmanteau} in the reverse direction.
\end{proof}
\end{document}
