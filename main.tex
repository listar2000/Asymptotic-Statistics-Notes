\documentclass{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{thmtools}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{float}
\usepackage{hyperref}
% \usepackage{bbold}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage[dvipsnames]{xcolor}
\usepackage{tcolorbox}
\usepackage{xcolor}
\usepackage{notes}  % Include the custom style file
% ------------------------------------------------------------------------------

\begin{document}

% ------------------------------------------------------------------------------
% Cover Page and ToC
% ------------------------------------------------------------------------------
\title{ \normalsize \textsc{}
		\\ [2.0cm]
		\HRule{1.5pt} \\
		\LARGE \textbf{\textit{Asymptotic Statistics} \\ \Large{by VAN DER VAART}
		\HRule{2.0pt} \\ \vspace*{10\baselineskip}}
		}
\date{5/24/2024 - ?}
\author{Scribed by:
\textbf{Sida Li}}

\maketitle
\newpage

\tableofcontents
\newpage

\section{Prelim: Probability and Measure}
\begin{note}
    This part follows Chapter 1 in Keener's \textit{Theoretical Statistics} textbook. It is meant to introduce some basic concepts that will be assumed throughout rest of the note. I also bring out necessary notations from here. \textbf{Precise definitions are usually ommitted, but can be found in the textbook.}
\end{note}

\subsection{Measure and Probability Space}

We start by refreshing the rigorous definition of \textbf{measure space}, then proceed with examples.

Given a set $\mathcal{X}$, a $\sigma$-\textbf{algebra} $\mathcal{A}$ is a collection of subsets of $\mathcal{X}$ that (1) contains $\mathcal{X}$ and the empty set $\emptyset$, (2) is closed under complements, and (3) is closed under countable unions. 

A \textbf{measure} $\mu:\mathcal{A} \rightarrow [0, \infty]$ is a function that assigns a non-negative real number to each element in $\mathcal{A}$, such that $\mu(\emptyset) = 0$ and $\mu$ is countably additive, i.e. for any disjoint sequence of sets $\{A_i\}_{i=1}^{\infty}$, 
\begin{equation}
    \mu\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} \mu(A_i)
\end{equation}
The triple $(\mathcal{X}, \mathcal{A}, \mu)$ is called a \textbf{measure space}. A measure $\nu$ such that $\nu(\mathcal{X}) = 1$ is called a \textbf{probability measure}, and then we can define a \textbf{probability space} $(\mathcal{X}, \mathcal{A}, \nu)$.

\subsection{Lebesgue Measure and Why We Care}

Many statistics textbook, including Keener's, assures the readers that ``measure theory is not needed'' -- the motivation behind this simplification is that most of the time, we are working with the \textbf{Lebesgue measure} on $\mathbb{R}^d$. Lebesgue measure is compatible with the usual notion of length, area, and volume (in 1, 2, and 3-dimensions), and is defined on the \textbf{Borel $\sigma$-algebra} $\mathcal{B}(\mathbb{R}^d)$, which is the smallest $\sigma$-algebra containing all open sets in $\mathbb{R}^d$. 

Another benefits of Lebesgue measure is its connection with our usual notion of integration. If a function $f$ is \textbf{Lebesgue integrable} (whose definition is detailed \href{https://math.stackexchange.com/questions/1716526/what-does-it-mean-to-say-that-a-function-is-integrable-with-respect-to-a-measure}{here}), then its integral is:
\begin{equation}
    \int_{\mathbb{R}^d} f(x)\, d\mu(x) = \int_{\mathbb{R}^d} f(x)\, dx
\end{equation}
where the RHS is the familiar Riemann integral. Throughout this notes, we consider all measures $\nu$ on $\mathbb{R}^d$\footnote{we also assume $\nu$ to be $\sigma$-finite, whose definition is skipped} to be \textbf{absolutely continuous} with respect to the Lebesgue measure $\mu$, i.e. $\mu(A) = 0$ implies $\nu(A) = 0$. Then by \textbf{Radonâ€“Nikodym's theorem}, we can write
\begin{equation}
    \int_{\mathbb{R}^d} f(x)\, d\nu(x) = \int_{\mathbb{R}^d} f(x) \frac{d\nu}{d\mu}(x)\, d\mu(x) = \int_{\mathbb{R}^d} f(x)\nu(x) \,dx
\end{equation}
for $\nu$-integrable functions $f$. Hopefully this clarifies why we can safely ignore measure theory in most of the (basic) statistics literature.


\newpage
\section{Convergence of Random Variables}

\begin{note}
	This part is largely following the main definitions and proofs in Chapter 2 of van der Vaart's book.
\end{note}
% ------------------------------------------------------------------------------

\subsection{Modes of Convergence}

We start by defining the different modes of convergence for random variables. Let $\{X_n\}_{n=1}^{\infty}$ be a sequence of random variables, and $X$ be another random variable. 

\begin{itemize}
    \item We say that $X_n$ \textbf{converges in distribution} to $X$ if
    \begin{equation}
        \lim_{n \rightarrow \infty} P(X_n \leq x) = P(X \leq x)
    \end{equation}
    for any $x$ such that the mapping $x \to P(X \leq x)$ is continuous. We denote this convergence as $X_n \xrightarrow{d} X$.
    
    \item We say $X_n$ \textbf{converges in probability} to $X$ if for any $\epsilon > 0$, 
    \begin{equation}
        \lim_{n \rightarrow \infty} P(d(X_n, X) > \epsilon) = 0
    \end{equation}
    where $d(\cdot, \cdot)$ is a distance metric between like Euclidean distance. We denote this convergence as $X_n \xrightarrow{P} X$.

    \item We say $X_n$ \textbf{converges almost surely} to $X$ if 
    \begin{equation}
        P\left(\lim_{n \rightarrow \infty} d(X_n, X) = 0  \right) = 1
    \end{equation}
    we also denote this convergence as $X_n \xrightarrow{a.s.} X$. This is considered as a stronger form of convergence than the two modes above.
\end{itemize}

\begin{lemma}
\label{lemma:portmanteau}
\textbf{(Portmanteau)} For any sequence of random variables $X_n$ and $X$ the following statements are equivalent.
\begin{itemize}
    \item[(i)] $P(X_n \leq x) \to P(X \leq x)$ for all continuity points of $x \mapsto P(X \leq x)$;
    \item[(ii)] $\E f(X_n) \to \E f(X)$ for all bounded, continuous functions $f$;
    \item[(iii)] $\E f(X_n) \to \E f(X)$ for all bounded, Lipschitz functions $f$;
    \item[(iv)] $\liminf \E f(X_n) \geq \E f(X)$ for all nonnegative, continuous functions $f$;
    \item[(v)] $\liminf P(X_n \in G) \geq P(X \in G)$ for every open set $G$;
    \item[(vi)] $\limsup P(X_n \in F) \leq P(X \in F)$ for every closed set $F$;
    \item[(vii)] $P(X_n \in B) \to P(X \in B)$ for all Borel sets $B$ with $P(X \in \delta B) = 0$, where $\delta B = \overline{B} - \overset{\circ}{B}$ is the boundary of $B$.
\end{itemize}
\end{lemma}

The textbook has proven all the equivalence except for $(ii) \Leftrightarrow (iv)$, which we give the proof below together with other proof supplements.

\begin{proof}[Proof supplements of Lemma 1]
    We first supplement $(i) \Rightarrow (ii)$ by proving that a continuous function on a compact set is uniformly continuous. Let $f$ be a continuous function on a compact domain $K$. By contradiction, if $f$ is not uniformly continuous, let $\epsilon > 0$, then for every $n \in \mathbb{N}$, there exists $x_n, y_n \in K$ such that $|x_n - y_n| < 1/n$ but $|f(x_n) - f(y_n)| \geq \epsilon$. Since $K$ is compact, we can extract a convergent subsequence $x_{n_k} \to x$ and $y_{n_k} \to y$. Then by continuity of $f$, we have $f(x) = \lim_{k \to \infty} f(x_{n_k}) = \lim_{k \to \infty} f(y_{n_k}) = f(y)$, which contradicts the assumption that $|f(x) - f(y)| \geq \epsilon$.

    Next we clarify $(iii) \Rightarrow (v)$ by explaining the Lipschitz approximation to the indicator function on open set $G$. Consider the sequence of Lipschitz functions $f_m$ defined as
    \begin{equation}
        f_m(x) = \begin{cases}
            1 & \text{if } d(x, G^c) \geq 1/m \\
            m \cdot d(x, G^c) & \text{if } d(x, G^c) < 1/m
        \end{cases}
    \end{equation}
    then $f_m$ is apparently Lipschitz with Lipschitz constant $m$. Since $f_m \uparrow \mathbf{1}_G$ pointwise, by $(iii)$ we have for every $m$
    \begin{equation}
        \liminf_{n\to\infty} P(X_n \in G) \geq \liminf _{n\to\infty}\E f_m(X_n) = \E f_m(X)
    \end{equation}
    and the RHS $\to P(X \in G)$ as $m \to \infty$.

    Finally, we prove $(ii) \Leftrightarrow (iv)$. Starting from the assumption $(ii)$. Let $f_m$ be a sequence of bounded, continuous functions such that $f_m \uparrow f$. We can expcliditly write $f_m$ as $f_m = \min(f, m)$. For each fixed $m$, we have
    \begin{equation}
        \liminf_{n\to\infty} \E f(X_n) \geq \liminf_{n\to\infty} \E f_m(X_n) = \E f_m(X)
    \end{equation}
    where the last equality is by $(ii)$. As $m \to \infty$, the RHS increases to $\E f(X)$ by the monotone convergence theorem. 
    
    For the reverse direction, let $m$ and $M$ be the lower and upper bounds of $f$, and define functions $g = f + m$ and $h = M - f$. Apparently both $g$ and $h$ are nonnegative, continuous functions. Then we have
    \begin{equation}
        \liminf_{n\to\infty} \E g(X_n) \geq \E g(X) \quad \text{and} \quad \liminf_{n\to\infty} \E h(X_n) \geq \E h(X)
    \end{equation}
    From the first inequality, we have $\liminf_{n\to\infty} \E f(X_n) \geq \E f(X)$. Similarly, from the second inequality, we have $\limsup_{n\to\infty} \E f(X_n) \leq \E f(X)$. Combining the two inequalities, we have $\liminf_{n\to\infty} \E f(X_n) = \E f(X)$.
\end{proof}

The next theorem is about how continuous mapping preserves all three modes of convergence mentioned above.

\begin{theorem}
    \label{thm:cont_map_thm}
    \textbf{(Continuous Mapping Theorem).} Let $g: \mathbb{R}^d \to \mathbb{R}^k$ be continuous at every point of a set \( C \) such that \( P(X \in C) = 1 \)
    \begin{enumerate}
    \item[(i)] If $X_n \xrightarrow{d} X$, then $g(X_n) \xrightarrow{d} g(X)$.
    \item[(ii)] If $X_n \xrightarrow{P} X$, then $g(X_n) \xrightarrow{P} g(X)$.
    \item[(iii)] If $X_n \xrightarrow{a.s.} X$, then $g(X_n) \xrightarrow{a.s.} g(X)$.
    \end{enumerate}   
\end{theorem}

\begin{proof}[Proof supplment of Theorem \ref*{thm:cont_map_thm}]
    For $(i)$, the easier way than the textbook is to realize that the preimage of a closed set under a continuous function is closed. Then we can utlize $(vi)$ in Lemma \ref{lemma:portmanteau} to prove $P(g(X_n) \in F) \to P(g(X) \in F)$ for every closed $F$.

    The $(iii)$ is trivial to prove: if the event $\lim_{n\to\infty}d(X_n, X) = 0$ is true, by continuity of $g$ we have $\lim_{n\to\infty}d(g(X_n), g(X)) = 0$ as well. Therefore:
    \begin{equation}
        P\left(\lim_{n\to\infty}d(g(X_n), g(X)) = 0\right) \geq P\left(\lim_{n\to\infty}d(X_n, X) = 0\right) = 1
    \end{equation}
\end{proof}
% Reference and Cited Works
% ------------------------------------------------------------------------------

% \bibliographystyle{apalike}
% \bibliography{References.bib}

% ------------------------------------------------------------------------------
For a sequence of random variables $\left\{ X_n \right\}$, the concept of \textbf{uniformly tightness} (also known as \textbf{bounded in probability}) states that for every $\epsilon > 0$, there exists a constant $M$ such that 
\begin{equation}
    \sup_{n} P(|X_n| > M) < \epsilon
\end{equation}
\textbf{Prohorov's theorem} then states the equivalence between uniformly tightness and convergence in distribution.
\begin{theorem}
    Let $\left\{ X_n \right\}$ be a sequence of random variables in $\mathbb{R}^k$.
    \begin{itemize}
        \item[(i)] If $X_n \xrightarrow{d} X$ for some $X$, then $\{X_n : n \in \mathbb{N}\}$ is uniformly tight;
        \item[(ii)] If $X_n$ is uniformly tight, then there exists a subsequence with $X_{n_j} \xrightarrow{d} X$ as $j \to \infty$, for some $X$.
    \end{itemize}
\end{theorem}
\begin{proof}[Note on the proof for $(i)$]
    There is a very trivial fact used in proving $(i)$, which we will elaborate here. We want to show that for any well-defined random variable $X$ and $\epsilon > 0$, there exists $M$ such that $P(|X| > M) < \epsilon$. This is equivalent to showing that $P(|X| \leq M) \geq 1 - \epsilon$. Since $P(|X| \leq M)$ is a monotonically increasing function of $M$, we can take $M$ to be the smallest value such that $P(|X| \leq M) \geq 1 - \epsilon$. We can also prove it using Markov's inequality on $|X|$.

    For the proof on $(ii)$, it relies on the following \textbf{Helley's Lemma}.
\end{proof}
\begin{lemma}[\textbf{Helley's Lemma}]
    Each given sequence $F_n$ of cumulative distribution functions on $\mathbb{R}^k$ possesses a subsequence $F_{n_j}$ with the property that $F_{n_j}(x) \to F(x)$ at each continuity point $x$ of a possibly \textbf{defective distribution function} $F$.
    \vspace*{0.2em}

    A \textbf{defective distribution function} is one that shares the same properties as a distribution function, except that its limits at infinities might be between 0 and 1.
\end{lemma}
\begin{proof}[Proof supplement] we skip over everything up to the diagonalization argument. Now we have a subsequence \( \left\{ F_{n_j} \right\}\) such that \(F_{n_j}(q_i) \to G(q_i)\) for each $q \in \mathbb{Q}$ (here for simplicity we deal with 1-dimensional case). We define 
\begin{equation}
    F(x) = \inf_{q > x} G(q)
\end{equation}
here we mainly supplement the ``monotonicity'' argument. If $x$ is a coninuity point of $F$, then we can always find rationals $q < x < q'$ such that $G(q) < F(x) < G(q')$ and $G(q') - G(q) < \epsilon$ for any $\epsilon > 0$. Now since $F_{n_j}(q) \to G(q)$ and same for $q'$, we have 
\begin{equation}
    G(q') = \liminf F_{n_j}(q') \geq \liminf F_{n_j}(x) \geq \liminf F_{n_j}(q) = G(q)
\end{equation}
which directly tells us that $\|\liminf F_{n_j}(x) - F(x)\| < \epsilon$. Similarly showing the $\limsup$ part gives us the desired result.
\end{proof}
\subsection{Connection Between Different Modes of Convergence}
The following theorem gives the connection between different modes of convergence.
\begin{theorem}
    Let $X_n, X$ and $Y_n$ be random vectors. Then
\begin{itemize}
    \item[(i)] $X_n \xrightarrow{a.s.} X$ implies $X_n \xrightarrow{P} X$;
    \item[(ii)] $X_n \xrightarrow{P} X$ implies $X_n \xrightarrow{d} X$;
    \item[(iii)] $X_n \xrightarrow{P} c$ for a constant $c$ if and only if $X_n \xrightarrow{d} c$;
    \item[(iv)] if $X_n \xrightarrow{d} X$ and $d(X_n, Y_n) \xrightarrow{P} 0$, then $Y_n \xrightarrow{d} X$;
    \item[(v)] if $X_n \xrightarrow{d} X$ and $Y_n \xrightarrow{P} c$ for a constant $c$, then $(X_n, Y_n) \xrightarrow{d} (X, c)$;
    \item[(vi)] if $X_n \xrightarrow{P} X$ and $Y_n \xrightarrow{P} Y$, then $(X_n, Y_n) \xrightarrow{P} (X, Y)$.
\end{itemize}
\end{theorem}
A quick conclusion is that convergence in distribution is the weakest form of convergence, while almost sure convergence is the strongest.
\begin{proof}[Proof supplement]\label{thm:connection}
    The proof for $(i)$ is trivial: if $X_n \xrightarrow{a.s.} X$, then for any $\epsilon > 0$ if we define $A_n = \left\{ k \geq n : d(X_k, X) > \epsilon \right\}$, we know that $A_n \downarrow \emptyset$ as $n \to \infty$. Therefore 
    \begin{equation}
        \lim_{n\to\infty} P(d(X_n, X) > \epsilon) \leq \lim_{n\to\infty} P(A_n) = 0
    \end{equation}
    it is worth noting that the last equality is by continuity of probability measure.

    For $(ii)$ we supplement a direct proof by showing that convergence in probability leads to $\E f(X_n) \to \E f(X)$ by Lemma \ref{lemma:portmanteau} for all bounded, continuous functions $f$. Firstly recall that by continuous mapping theorem, if $f$ is continuous, then $f(X_n) \xrightarrow{P} f(X)$, i.e.
    \begin{equation}
        \lim_{n\to\infty} P(|f(X_n) - f(X)| > \epsilon) = 0
    \end{equation}
    for any fixed $\epsilon > 0$. As a result, we have
    \begin{equation}
        \lim_{n\to\infty} \E |f(X_n) - f(X)| \leq \epsilon
    \end{equation}
    since $\epsilon$ is arbitrary, we have $\E |f(X_n) - f(X)| \to 0$. This is equivalent to $\E f(X_n) \to \E f(X)$.
\end{proof}
The $(v)$ in Theorem $\ref{thm:connection}$ is especially useful in proving the \textbf{Slutsky's lemma}.
\begin{lemma}[Slutsky's Lemma]
    Let $X_n, X$ and $Y_n$ be random variables. If $X_n \xrightarrow{d} X$ and $Y_n \xrightarrow{d} c$ for a constant $c$, then
    \begin{enumerate}
        \item[(i)] $X_n + Y_n \xrightarrow{d} X + c$
        \item[(ii)] $X_n Y_n \xrightarrow{d} X \cdot c$
        \item[(iii)] $X_n / Y_n \xrightarrow{d} X / c$ if $c \neq 0$.
    \end{enumerate}
\end{lemma}
In each of the cases above, we leverage points $(iii), (v)$ in $\ref{thm:connection}$ first, then define a continuous mapping $f(X_n, Y_n)$ to enable continuous mapping.
\end{document}
