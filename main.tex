\documentclass{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{thmtools}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{float}
\usepackage{hyperref}
% \usepackage{bbold}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage[dvipsnames]{xcolor}
\usepackage{tcolorbox}
\usepackage{xcolor}
\usepackage{notes}  % Include the custom style file
% ------------------------------------------------------------------------------

\begin{document}

% ------------------------------------------------------------------------------
% Cover Page and ToC
% ------------------------------------------------------------------------------
\title{ \normalsize \textsc{}
		\\ [2.0cm]
		\HRule{1.5pt} \\
		\LARGE \textbf{\textit{Asymptotic Statistics} \\ \Large{by VAN DER VAART}
		\HRule{2.0pt} \\ \vspace*{10\baselineskip}}
		}
\date{5/24/2024 - ?}
\author{Scribed by:
\textbf{Sida Li}}

\maketitle
\newpage

\tableofcontents
\newpage

\section{Prelim: Probability and Measure}
\begin{note}
    This part follows Chapter 1 in Keener's \textit{Theoretical Statistics} textbook. It is meant to introduce some basic concepts that will be assumed throughout rest of the note. I also bring out necessary notations from here. \textbf{Precise definitions are usually ommitted, but can be found in the textbook.}
\end{note}

\subsection{Measure and Probability Space}

We start by refreshing the rigorous definition of \textbf{measure space}, then proceed with examples.

Given a set $\mathcal{X}$, a $\sigma$-\textbf{algebra} $\mathcal{A}$ is a collection of subsets of $\mathcal{X}$ that (1) contains $\mathcal{X}$ and the empty set $\emptyset$, (2) is closed under complements, and (3) is closed under countable unions. 

A \textbf{measure} $\mu:\mathcal{A} \rightarrow [0, \infty]$ is a function that assigns a non-negative real number to each element in $\mathcal{A}$, such that $\mu(\emptyset) = 0$ and $\mu$ is countably additive, i.e. for any disjoint sequence of sets $\{A_i\}_{i=1}^{\infty}$, 
\begin{equation}
    \mu\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} \mu(A_i)
\end{equation}
The triple $(\mathcal{X}, \mathcal{A}, \mu)$ is called a \textbf{measure space}. A measure $\nu$ such that $\nu(\mathcal{X}) = 1$ is called a \textbf{probability measure}, and then we can define a \textbf{probability space} $(\mathcal{X}, \mathcal{A}, \nu)$.

\subsection{Lebesgue Measure and Why We Care}

Many statistics textbook, including Keener's, assures the readers that ``measure theory is not needed'' -- the motivation behind this simplification is that most of the time, we are working with the \textbf{Lebesgue measure} on $\mathbb{R}^d$. Lebesgue measure is compatible with the usual notion of length, area, and volume (in 1, 2, and 3-dimensions), and is defined on the \textbf{Borel $\sigma$-algebra} $\mathcal{B}(\mathbb{R}^d)$, which is the smallest $\sigma$-algebra containing all open sets in $\mathbb{R}^d$. 

Another benefits of Lebesgue measure is its connection with our usual notion of integration. If a function $f$ is \textbf{Lebesgue integrable} (whose definition is detailed \href{https://math.stackexchange.com/questions/1716526/what-does-it-mean-to-say-that-a-function-is-integrable-with-respect-to-a-measure}{here}), then its integral is:
\begin{equation}
    \int_{\mathbb{R}^d} f(x)\, d\mu(x) = \int_{\mathbb{R}^d} f(x)\, dx
\end{equation}
where the RHS is the familiar Riemann integral. Throughout this notes, we consider all measures $\nu$ on $\mathbb{R}^d$\footnote{we also assume $\nu$ to be $\sigma$-finite, whose definition is skipped} to be \textbf{absolutely continuous} with respect to the Lebesgue measure $\mu$, i.e. $\mu(A) = 0$ implies $\nu(A) = 0$. Then by \textbf{Radonâ€“Nikodym's theorem}, we can write
\begin{equation}
    \int_{\mathbb{R}^d} f(x)\, d\nu(x) = \int_{\mathbb{R}^d} f(x) \frac{d\nu}{d\mu}(x)\, d\mu(x) = \int_{\mathbb{R}^d} f(x)\nu(x) \,dx
\end{equation}
for $\nu$-integrable functions $f$. Hopefully this clarifies why we can safely ignore measure theory in most of the (basic) statistics literature.


\newpage
\section{Convergence of Random Variables}

\begin{note}
	This part is largely following the main definitions and proofs in Chapter 2 of van der Vaart's book.
\end{note}
% ------------------------------------------------------------------------------

\subsection{Modes of Convergence}

We start by defining the different modes of convergence for random variables. Let $\{X_n\}_{n=1}^{\infty}$ be a sequence of random variables, and $X$ be another random variable. 

\begin{itemize}
    \item We say that $X_n$ \textbf{converges in distribution} to $X$ if
    \begin{equation}
        \lim_{n \rightarrow \infty} P(X_n \leq x) = P(X \leq x)
    \end{equation}
    for any $x$ such that the mapping $x \to P(X \leq x)$ is continuous. We denote this convergence as $X_n \xrightarrow{d} X$.
    
    \item We say $X_n$ \textbf{converges in probability} to $X$ if for any $\epsilon > 0$, 
    \begin{equation}
        \lim_{n \rightarrow \infty} P(d(X_n, X) > \epsilon) = 0
    \end{equation}
    where $d(\cdot, \cdot)$ is a distance metric between like Euclidean distance. We denote this convergence as $X_n \xrightarrow{P} X$.

    \item We say $X_n$ \textbf{converges almost surely} to $X$ if 
    \begin{equation}
        P\left(\lim_{n \rightarrow \infty} d(X_n, X) = 0  \right) = 1
    \end{equation}
    we also denote this convergence as $X_n \xrightarrow{a.s.} X$. This is considered as a stronger form of convergence than the two modes above.
\end{itemize}

\begin{lemma}
\label{lemma:portmanteau}
\textbf{(Portmanteau)} For any sequence of random variables $X_n$ and $X$ the following statements are equivalent.
\begin{itemize}
    \item[(i)] $P(X_n \leq x) \to P(X \leq x)$ for all continuity points of $x \mapsto P(X \leq x)$;
    \item[(ii)] $\E f(X_n) \to \E f(X)$ for all bounded, continuous functions $f$;
    \item[(iii)] $\E f(X_n) \to \E f(X)$ for all bounded, Lipschitz functions $f$;
    \item[(iv)] $\liminf \E f(X_n) \geq \E f(X)$ for all nonnegative, continuous functions $f$;
    \item[(v)] $\liminf P(X_n \in G) \geq P(X \in G)$ for every open set $G$;
    \item[(vi)] $\limsup P(X_n \in F) \leq P(X \in F)$ for every closed set $F$;
    \item[(vii)] $P(X_n \in B) \to P(X \in B)$ for all Borel sets $B$ with $P(X \in \delta B) = 0$, where $\delta B = \overline{B} - \overset{\circ}{B}$ is the boundary of $B$.
\end{itemize}
\end{lemma}

The textbook has proven all the equivalence except for $(ii) \Leftrightarrow (iv)$, which we give the proof below together with other proof supplements.

\begin{proof}[Proof supplements of Lemma 1]
    We first supplement $(i) \Rightarrow (ii)$ by proving that a continuous function on a compact set is uniformly continuous. Let $f$ be a continuous function on a compact domain $K$. By contradiction, if $f$ is not uniformly continuous, let $\epsilon > 0$, then for every $n \in \mathbb{N}$, there exists $x_n, y_n \in K$ such that $|x_n - y_n| < 1/n$ but $|f(x_n) - f(y_n)| \geq \epsilon$. Since $K$ is compact, we can extract a convergent subsequence $x_{n_k} \to x$ and $y_{n_k} \to y$. Then by continuity of $f$, we have $f(x) = \lim_{k \to \infty} f(x_{n_k}) = \lim_{k \to \infty} f(y_{n_k}) = f(y)$, which contradicts the assumption that $|f(x) - f(y)| \geq \epsilon$.

    Next we clarify $(iii) \Rightarrow (v)$ by explaining the Lipschitz approximation to the indicator function on open set $G$. Consider the sequence of Lipschitz functions $f_m$ defined as
    \begin{equation}
        f_m(x) = \begin{cases}
            1 & \text{if } d(x, G^c) \geq 1/m \\
            m \cdot d(x, G^c) & \text{if } d(x, G^c) < 1/m
        \end{cases}
    \end{equation}
    then $f_m$ is apparently Lipschitz with Lipschitz constant $m$. Since $f_m \uparrow \mathbf{1}_G$ pointwise, by $(iii)$ we have for every $m$
    \begin{equation}
        \liminf_{n\to\infty} P(X_n \in G) \geq \liminf _{n\to\infty}\E f_m(X_n) = \E f_m(X)
    \end{equation}
    and the RHS $\to P(X \in G)$ as $m \to \infty$.

    Finally, we prove $(ii) \Leftrightarrow (iv)$. Starting from the assumption $(ii)$. Let $f_m$ be a sequence of bounded, continuous functions such that $f_m \uparrow f$. We can expcliditly write $f_m$ as $f_m = \min(f, m)$. For each fixed $m$, we have
    \begin{equation}
        \liminf_{n\to\infty} \E f(X_n) \geq \liminf_{n\to\infty} \E f_m(X_n) = \E f_m(X)
    \end{equation}
    where the last equality is by $(ii)$. As $m \to \infty$, the RHS increases to $\E f(X)$ by the monotone convergence theorem. 
    
    For the reverse direction, let $m$ and $M$ be the lower and upper bounds of $f$, and define functions $g = f + m$ and $h = M - f$. Apparently both $g$ and $h$ are nonnegative, continuous functions. Then we have
    \begin{equation}
        \liminf_{n\to\infty} \E g(X_n) \geq \E g(X) \quad \text{and} \quad \liminf_{n\to\infty} \E h(X_n) \geq \E h(X)
    \end{equation}
    From the first inequality, we have $\liminf_{n\to\infty} \E f(X_n) \geq \E f(X)$. Similarly, from the second inequality, we have $\limsup_{n\to\infty} \E f(X_n) \leq \E f(X)$. Combining the two inequalities, we have $\liminf_{n\to\infty} \E f(X_n) = \E f(X)$.
\end{proof}

The next theorem is about how continuous mapping preserves all three modes of convergence mentioned above.

\begin{theorem}
    \label{thm:cont_map_thm}
    \textbf{(Continuous Mapping Theorem).} Let $g: \mathbb{R}^d \to \mathbb{R}^k$ be continuous at every point of a set \( C \) such that \( P(X \in C) = 1 \)
    \begin{enumerate}
    \item[(i)] If $X_n \xrightarrow{d} X$, then $g(X_n) \xrightarrow{d} g(X)$.
    \item[(ii)] If $X_n \xrightarrow{P} X$, then $g(X_n) \xrightarrow{P} g(X)$.
    \item[(iii)] If $X_n \xrightarrow{a.s.} X$, then $g(X_n) \xrightarrow{a.s.} g(X)$.
    \end{enumerate}   
\end{theorem}

\begin{proof}[Proof supplment of Theorem \ref*{thm:cont_map_thm}]
    For $(i)$, the easier way than the textbook is to realize that the preimage of a closed set under a continuous function is closed. Then we can utlize $(vi)$ in Lemma \ref{lemma:portmanteau} to prove $P(g(X_n) \in F) \to P(g(X) \in F)$ for every closed $F$.

    The $(iii)$ is trivial to prove: if the event $\lim_{n\to\infty}d(X_n, X) = 0$ is true, by continuity of $g$ we have $\lim_{n\to\infty}d(g(X_n), g(X)) = 0$ as well. Therefore:
    \begin{equation}
        P\left(\lim_{n\to\infty}d(g(X_n), g(X)) = 0\right) \geq P\left(\lim_{n\to\infty}d(X_n, X) = 0\right) = 1
    \end{equation}
\end{proof}
% Reference and Cited Works
% ------------------------------------------------------------------------------

% \bibliographystyle{apalike}
% \bibliography{References.bib}

% ------------------------------------------------------------------------------
For a sequence of random variables $\left\{ X_n \right\}$, the concept of \textbf{uniformly tightness} (also known as \textbf{bounded in probability}) states that for every $\epsilon > 0$, there exists a constant $M$ such that 
\begin{equation}
    \sup_{n} P(|X_n| > M) < \epsilon
\end{equation}
\textbf{Prohorov's theorem} then states the equivalence between uniformly tightness and convergence in distribution.
\begin{theorem}
    Let $\left\{ X_n \right\}$ be a sequence of random variables in $\mathbb{R}^k$.
    \begin{itemize}
        \item[(i)] If $X_n \xrightarrow{d} X$ for some $X$, then $\{X_n : n \in \mathbb{N}\}$ is uniformly tight;
        \item[(ii)] If $X_n$ is uniformly tight, then there exists a subsequence with $X_{n_j} \xrightarrow{d} X$ as $j \to \infty$, for some $X$.
    \end{itemize}
\end{theorem}
\begin{proof}[Note on the proof for $(i)$]
    There is a very trivial fact used in proving $(i)$, which we will elaborate here. We want to show that for any well-defined random variable $X$ and $\epsilon > 0$, there exists $M$ such that $P(|X| > M) < \epsilon$. This is equivalent to showing that $P(|X| \leq M) \geq 1 - \epsilon$. Since $P(|X| \leq M)$ is a monotonically increasing function of $M$, we can take $M$ to be the smallest value such that $P(|X| \leq M) \geq 1 - \epsilon$. We can also prove it using Markov's inequality on $|X|$.

    For the proof on $(ii)$, it relies on the following \textbf{Helley's Lemma}.
\end{proof}
\begin{lemma}[\textbf{Helley's Lemma}]
    Each given sequence $F_n$ of cumulative distribution functions on $\mathbb{R}^k$ possesses a subsequence $F_{n_j}$ with the property that $F_{n_j}(x) \to F(x)$ at each continuity point $x$ of a possibly \textbf{defective distribution function} $F$.
    \vspace*{0.2em}

    A \textbf{defective distribution function} is one that shares the same properties as a distribution function, except that its limits at infinities might be between 0 and 1.
\end{lemma}
\begin{proof}[Proof supplement] we skip over everything up to the diagonalization argument. Now we have a subsequence \( \left\{ F_{n_j} \right\}\) such that \(F_{n_j}(q_i) \to G(q_i)\) for each $q \in \mathbb{Q}$ (here for simplicity we deal with 1-dimensional case). We define 
\begin{equation}
    F(x) = \inf_{q > x} G(q)
\end{equation}
here we mainly supplement the ``monotonicity'' argument. If $x$ is a coninuity point of $F$, then we can always find rationals $q < x < q'$ such that $G(q) < F(x) < G(q')$ and $G(q') - G(q) < \epsilon$ for any $\epsilon > 0$. Now since $F_{n_j}(q) \to G(q)$ and same for $q'$, we have 
\begin{equation}
    G(q') = \liminf F_{n_j}(q') \geq \liminf F_{n_j}(x) \geq \liminf F_{n_j}(q) = G(q)
\end{equation}
which directly tells us that $\|\liminf F_{n_j}(x) - F(x)\| < \epsilon$. Similarly showing the $\limsup$ part gives us the desired result.
\end{proof}
\subsection{Connection Between Different Modes of Convergence}
The following theorem gives the connection between different modes of convergence.
\begin{theorem}
    Let $X_n, X$ and $Y_n$ be random vectors. Then
\begin{itemize}
    \item[(i)] $X_n \xrightarrow{a.s.} X$ implies $X_n \xrightarrow{P} X$;
    \item[(ii)] $X_n \xrightarrow{P} X$ implies $X_n \xrightarrow{d} X$;
    \item[(iii)] $X_n \xrightarrow{P} c$ for a constant $c$ if and only if $X_n \xrightarrow{d} c$;
    \item[(iv)] if $X_n \xrightarrow{d} X$ and $d(X_n, Y_n) \xrightarrow{P} 0$, then $Y_n \xrightarrow{d} X$;
    \item[(v)] if $X_n \xrightarrow{d} X$ and $Y_n \xrightarrow{P} c$ for a constant $c$, then $(X_n, Y_n) \xrightarrow{d} (X, c)$;
    \item[(vi)] if $X_n \xrightarrow{P} X$ and $Y_n \xrightarrow{P} Y$, then $(X_n, Y_n) \xrightarrow{P} (X, Y)$.
\end{itemize}
\end{theorem}
A quick conclusion is that convergence in distribution is the weakest form of convergence, while almost sure convergence is the strongest.
\begin{proof}[Proof supplement]\label{thm:connection}
    The proof for $(i)$ is trivial: if $X_n \xrightarrow{a.s.} X$, then for any $\epsilon > 0$ if we define $A_n = \left\{ k \geq n : d(X_k, X) > \epsilon \right\}$, we know that $A_n \downarrow \emptyset$ as $n \to \infty$. Therefore 
    \begin{equation}
        \lim_{n\to\infty} P(d(X_n, X) > \epsilon) \leq \lim_{n\to\infty} P(A_n) = 0
    \end{equation}
    it is worth noting that the last equality is by continuity of probability measure.

    For $(ii)$ we supplement a direct proof by showing that convergence in probability leads to $\E f(X_n) \to \E f(X)$ by Lemma \ref{lemma:portmanteau} for all bounded, continuous functions $f$. Firstly recall that by continuous mapping theorem, if $f$ is continuous, then $f(X_n) \xrightarrow{P} f(X)$, i.e.
    \begin{equation}
        \lim_{n\to\infty} P(|f(X_n) - f(X)| > \epsilon) = 0
    \end{equation}
    for any fixed $\epsilon > 0$. As a result, we have
    \begin{equation}
        \lim_{n\to\infty} \E |f(X_n) - f(X)| \leq \epsilon
    \end{equation}
    since $\epsilon$ is arbitrary, we have $\E |f(X_n) - f(X)| \to 0$. This is equivalent to $\E f(X_n) \to \E f(X)$.
\end{proof}
The $(v)$ in Theorem $\ref{thm:connection}$ is especially useful in proving the \textbf{Slutsky's lemma}.
\begin{lemma}[Slutsky's Lemma]
    Let $X_n, X$ and $Y_n$ be random variables. If $X_n \xrightarrow{d} X$ and $Y_n \xrightarrow{d} c$ for a constant $c$, then
    \begin{enumerate}
        \item[(i)] $X_n + Y_n \xrightarrow{d} X + c$
        \item[(ii)] $X_n Y_n \xrightarrow{d} X \cdot c$
        \item[(iii)] $X_n / Y_n \xrightarrow{d} X / c$ if $c \neq 0$.
    \end{enumerate}
\end{lemma}
In each of the cases above, we leverage points $(iii), (v)$ in $\ref{thm:connection}$ first, then define a continuous mapping $f(X_n, Y_n)$ to enable continuous mapping.

Finally, if the distribution function of the limit $X$ is continuous, then convergence in distribution implies uniform convergence of $P(X_n < x) \to P(X < x)$ for every $x$.
\begin{lemma}
    Let $X_n$ and $X$ be random variables and $X$ has a continuous distribution function. If $X_n \xrightarrow{d} X$, then $P(X_n < x) \to P(X < x)$ uniformly in $x$.
\end{lemma}
\begin{proof}[Proof for $d = 2$]
    Here we supplement a proof for the 2-dimensional case. Let $F$ and $F_n$ denotes the distribution function of $X$ and $X_n$ respectively. By continuity of $F$ we can find $k$ points $-\infty = a_1 < a_2 < \cdots < a_{k} = \infty$ on the first coordinate and another $k$ points $-\infty = b_1 < b_2 < \cdots < b_{k} = \infty$ on the second coordinate such that $F(a_i, b_j) = (ij)/k^2$.

    Now consider any point $(a, b) \in \mathbb{R}^2$, we can find $i, j$ such that $a \in [a_i, a_{i+1}]$ and $b \in [b_j, b_{j+1}]$. We thus have the inequalities
    \begin{align*}
        F_n(a, b) - F(a, b) &\leq F_n(a_{i+1}, b_{j+1}) - F(a_i, b_j) \\ &= F_n(a_{i+1}, b_{j+1}) - F(a_{i+1}, b_{j+1}) + (i + j + 1)/k^2 
        \\ &\geq F_n(a_{i}, b_{j}) - F(a_{i+1}, b_{j+1})
        \\ &= F_n(a_{i}, b_{j}) - F(a_{i}, b_{j}) - (i + j + 1)/k^2
    \end{align*}
    the rest is the same as the 1-dimensional case since $F_n \to F$ and $(i + j + 1)/k^2 \leq (2k + 1)/k^2$ will vanish as $k \to \infty$.
\end{proof}

\subsection{Stochastic Big-O and Little-o Notations}
\begin{definition}
    The $O_p$ (Big-O) notation denotes how a sequence of random variables are bounded in probability. We say $X_n = O_p(a_n)$ if for every $\epsilon > 0$, there exists finite $M, N > 0$ such that
    \begin{equation}
        \sup_{m \geq N} P\left( \left|\frac{X_m}{a_m}\right| > M \right) < \epsilon
    \end{equation}
\end{definition}
\begin{definition}
    The $o_p$ (Little-o) notation denotes how a sequence of random variables converges to 0 in probability. We say $X_n = o_p(a_n)$ if for every $\epsilon > 0$, we have
    \begin{equation}
        \lim_{n \to \infty}P\left( \left|\frac{X_n}{a_n}\right| > \epsilon \right) = 0
    \end{equation}
    which is essentially $\frac{X_n}{a_n} \xrightarrow{P} 0$.
\end{definition}
A few important results can be shown using the definitions. The commonly used ones are:
\begin{enumerate}
    \item If $X_n = O_p(1)$ and $Y_n = O_p(1)$, then $X_n + Y_n = O_p(1)$;
    \item If $X_n = O_p(1)$ and $Y_n = o_p(1)$, then $X_nY_n = o_p(1)$;
    \item If $X_n = o_p(1)$ and $Y_n = o_p(1)$, then $X_n + Y_n = o_p(1)$.
    \item If $X_n = O_p(1)$ and $Y_n = O_p(1)$, then $X_nY_n = O_p(1)$.
\end{enumerate}
For the fourth point, we can easily find $M_1, M_2, N_1, N_2$ such that
\begin{equation}
    P\left( \left|X_n\right| > M_1 \right) < \epsilon \quad \text{and} \quad P\left( \left|Y_n\right| > M_2 \right) < \epsilon
\end{equation}
for $n \geq \max{N_1, N_2}$. Then we have
\begin{equation}
    P\left( \left|X_nY_n\right| > M_1M_2 \right) \leq P\left( \left|X_n\right| > M_1 \right) \cdot P\left( \left|Y_n\right| > M_2 \right) < \epsilon^2
\end{equation}
which easily implies $X_nY_n = O_p(1)$ since $\epsilon$ is arbitrary.
\newpage
% Below is chapter 6
\setcounter{section}{5}
\section{Contiguity}
\begin{note}
In probability theory, two sequences of probability measures are said to be \textbf{contiguous} if asymptotically they share the same support. Thus the notion of \textbf{contiguity} extends the concept of absolute continuity to the sequences of measures.

The concept was originally introduced by \textbf{Le Cam} (1960) as part of his foundational contribution to the development of asymptotic theory in mathematical statistics. This section blends in VDV's textbook and Peter Barlett's STAT 210B lecture notes\footnotemark.
\end{note}
\footnotetext{https://www.stat.berkeley.edu/~bartlett/courses/2013spring-stat210b/notes/21notes.pdf}

\subsection{Likelihood Ratios (Radon-Nikodym Derivatives)}
We start by considering two probability measures $P$ and $Q$ on a common measurable space $(\Omega, \mathcal{A})$. The definitions of \textbf{absolutely continuous} and \textbf{orthogonal} (singular) for measures are as follows:
\begin{definition}
    Measure $P$ is \textbf{absolutely continuous} with respect to $Q$ if $Q(A) = 0$ implies $P(A) = 0$ for all $A \in \mathcal{A}$. We denote this as $P \ll Q$.
\end{definition}
\begin{definition}
    Measure $P$ is \textbf{orthogonal} (singular) with respect to $Q$ if there exists a set $A \in \mathcal{A}$ such that $Q(A) = 0$ and $P(A^c) = 0$. We denote this as $P \perp Q$.
\end{definition}
With these definitions, we can start look into the densities. Let $p$ and $q$ be the densities of $P$ and $Q$ with respect to a common measure $\mu$ (Lebesgue measure). Consider the set $\Omega_P = \left\{ \omega : p(\omega) > 0 \right\}$ and $\Omega_Q = \left\{ \omega : q(\omega) > 0 \right\}$. Then by defintion, we can write measure $Q$ as the sum $Q = Q^a + Q^\perp$ where
\begin{equation}
    Q^a(A) = Q(A\cap \Omega_p) \quad \text{and} \quad Q^\perp(A) = Q(A\cap \Omega_p^c)
\end{equation}
for any measurable set $A$. This is also called the \textbf{Lebesgue decomposition} of $Q$ with respect to $P$. Using these definitions, we have the following lemma:
\begin{lemma}
    Let \(P\) and \(Q\) be probability measures with densities \(p\) and \(q\) with respect to a measure \(\mu\).
\begin{enumerate}
    \item[(i)] \(Q = Q^a + Q^\perp\), \(Q^a \ll P\), \(Q^\perp \perp P\).
    \item[(ii)] \(Q^a(A) = \int_A \left(q/p\right) dP\) for every measurable set \(A\).
    \item[(iii)] \(Q \ll P \:\Longleftrightarrow\: Q(\{p = 0\}) = 0 \:\Longleftrightarrow\: \int \left(q/p\right) dP = 1\).
\end{enumerate}
\end{lemma}\label{lemma:contiguity}
\begin{proof}[Proof supplement]
    Here's a clearer proof for $Q^a \ll P$. Consider any $\mathcal{O} \in \mathcal{A}$ such that $P(\mathcal{O})$. Thus we have $\mu(\mathcal{O}\cap\Omega_P) = \mu(\{x \in \mathcal{O}: p(x) > 0\}) = 0$. Thus $Q^\perp(\mathcal{O}) = 0$ since $\mu$ is the dominating measure.
\end{proof}
The intuition behind $(iii)$ in Lemma \ref{lemma:contiguity} is immense. For any two measures $P$ and $Q$, very likeliy they are neither absolutely continuous nor orthogonal. However, there is always a way to ``generate'' a new measure $Q^a$ from $Q$ that is absolutely continuous with respect to $P$ (and vice versa). $Q^a$ might not be a valid probability measure, but when it does, we know that $Q$ is absolutely continuous with respect to $P$.

In statistics, the Radon-Nikodym derivative $q/p$ is the density of $Q^a$ with respect to $P$, as illustrated in \ref{lemma:contiguity} $(ii)$. However, people often denotes:
\begin{equation}
    \frac{dQ}{dP} := \frac{dQ^a}{dP} = \frac{q}{p} \quad P - a.s.
\end{equation}
which is only $P$-almost surely unique, and we can feel free to define the quotient for $p = 0$. In general, for nonnegative measurable function $f$ and arbitrary measures $P$ and $Q$:
\begin{equation}
    \int f \, dQ \geq \int_{p>0} f q \, d\mu = \int_{p>0} f \frac{q}{p} p \, d\mu = \int f \frac{dQ}{dP} \, dP
\end{equation}

\subsection{Contiguity of Sequences of Measures}
\end{document}
